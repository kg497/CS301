# Project Name: (US Patent Phrase to Phrase Matching)
## Project Summary: (This project will output similarity scores for two phrases based on a context.)
## Github Link: 
## Group Names: KellyAnne George, Jillian Jacinto, Evelyn Garcia Mendoza
## Kaggle Competition Link: 
- Abstract:
  -  The project aims to determine the similarity score between two phrases to understand if they share meaning. Finding meaning between two phrases will help speed up the process of vetting and granting US Patents. It will reduce the strain on personnel who need to go over the large quantities of existing patents to compare the language in new patents. For our approach, we used word embeddings to create vectors that hold values of other words that are correlated with that word. For example, if you had the sentence “the dog likes walks,” then each word would be given an index from 0 to 3. Then a vector for the word dog would then contain the indexes 0, 2, 3. When comparing two words, such as “dog” and “walks,” it would be easy to determine that they share some correlation. The less correlation two words share, the less semantic similarity they have. Overall, our results show that this method does improve the accuracy of finding the semantic similarity but still does not produce good results.
- Introduction:
  - The project described is based on the Kaggle Competition, “US Patent Phrase to Phrase Matching.” The problem emphasizes the need to understand the similarity between patents because of the mass amount of patents that have been created and the limited amount of human resources to through the vetting process. To simplify this problem, two columns are given in the data that would represent two patents. On top of the two phrases, a context is given to describe the phrases' category. This is needed because two phrases can have similar or different meanings depending on the overall context. Afterward, the output should be a number from 0 to 1, representing the similarity of the two phrases. A score of 0 would mean that the phrases have no similarity, 0.25 would mean related in terms of domain but not the same meaning, 0.5 would mean the phrases are synonyms but do not have the same meaning, 0.75 means that the phrases are close synonyms, and 1.0 would mean they are exact matches other than conjugation, quantity, and words like “the,” “and,” “or,” etc. Semantic similarity is useful for understanding the meaning of words in different contexts. This would be helpful for tasks such as an AI assistant like Siri or Alexa, which uses Natural Language Processing to determine meaning from the speaker. Also, search engines could use semantic similarity to come up with the best results by finding output with similar meaning to the actual phrase searched. Of course, a huge impact would be on patent development. The better semantic similarity detection the US Patents have, the easier it will be to detect if a patent should be approved or not. This could reduce delay times in getting a patent and reduce labor to compare patents. 
- Related Work:
  - There are several methods out there attempting to solve the problem of patent phrase matching. The process involves both qualitative and quantitative research. In the most direct approach, traditional methods do not include deep learning approaches and focus on the excavation of specific features for classification in patents. However, more modern methods build off the non-deep learning, such as the association rule or the support vector machine, and enhance them with machine learning methods. The association rule is widely used to discover relations between variables in large data sets. It also gives way to identifying the likelihood of variables being connected. The process of the association rule is exploring the data for if-then patterns using points of reference that show how frequently a variable appears in a data set and the confidence being how many times the result concludes to be true. Although the association rule aids in finding and understanding patterns, it finds a large number of rules that could not be relevant and, therefore, result in low performance. Support vector machines (SVMs) are supervised learning models that are used to analyze data for classification and regression analysis. Although SVMs can classify both linear and nonlinear functions like neural networks, SVMs do not work on a continuous function and work on a hyperplane that approximates the most promising classification. Potential drawbacks to SVM are the need for full labeling on input data, hyperplanes can be skewed with a polar data set, and it requires a lot of time when testing large data sets.  
  - One method that is commonly used for semantic similarity detection is word2vec. This algorithm takes in data, such as sentences or passages, and creates vectors for each word containing associated words, represented by numbers. After creating these vectors, one could take the dot product of the vector for two words in order to compare the similarity. A higher dot product corresponds to a higher similarity between the words. Afterwards, the vectors could be used to train a neural network. This neural network will be based on the CBOW (Continuous Bag of Words) algorithm. This architecture uses the surrounding words to determine the input word. Our approach is similar in that we used word embeddings to compare the similarity between the two phrases. However, our approach used GloVe embedding instead of word2vec. While the two share similar purposes, word2vec allows for local context whereas GloVe uses global data. In this scenario, word2vec would be better since it would more easily be able to account for the different contexts we are given in the project. 
  - A deep learning algorithm that could tie well to the embedding method is the Convolutional Neural Network or ConvNet (CNN). CNN takes input data and assigns weight and biases to the characteristic of the input data while being able to differentiate one from another. The input data goes through hidden layers known as convolution layers (a join of two sets of information) to produce a third relation. The convolution layer detects the patterns using filters/kernels. These filters are a matrix of a size determined by the researcher initialized to random numbers. It slides or convolves through the data by the given matrix and generates a dot product that will be passed to the next layer as input data; the filters will detect patterns. There could be more than one filter to a layer, and it is also essential to determine the stride of each slide. However, if not careful, CNN can overfit, and instead of learning from the training data, the model will memorize. Using CNN to classify text will need a focus on the whole word and similarities,  applying the dot product of the embedded word vectors, which will help find the relationship between words in a specific class. 
  - Another method that was used is implementing BERT (Bidirectional Encoder Representations from Transformers) which is an AI algorithm created by Google to help find better search results. BERT helps with NLP pretraining in order to understand the context of words to come up with better search results. This could be used in our patent project by using the contexts provided to differentiate phrases. The reason we did not implement BERT is because it would take away from us doing the pretraining on our own.
- Data:
  - In the given data, there are two files used called ‘train.csv’ and ‘test.csv’. These files contain columns for ‘anchor’ and ‘target’ which hold the two phrases that need to be compared. There is also a column ‘context’, which holds the CPC classification that the phrases need to be compared based on. Finally, the train data contains the similarity score, a value from 0 to 1. Each CPC classification has a specific meaning behind it. 
  - ![image](https://user-images.githubusercontent.com/90022095/165000117-198d118a-6782-47b9-8510-1c6488ff7750.png)
  - This graph shows the distribution of scores from 0 to 1 from the ‘train.csv’ file. From this graph, it can be seen that most of the data falls in the categories of 0.25 and 0.5. Evidently, training the data with values originally closer to .25 and .5 will give us better results for the baseline of our neural network.
  - Outside of the given data, we also used ‘titles.csv’ which gave more depth into the meanings behind the CPC classifications. This would help account for the lack of using word2vec in our processing. For this file, we did some preprocessing to remove unnecessary characters including commas, colons, semicolons, and braces and replaced them with spaces. We also made all of the characters lowercase in order to clean up the data. Afterwards, we removed syncategorematic words that add no meaning to the words. 
  - Finally we had GloVe files which contained word vectors with all the indexes that correlated with the word. This data was very large, coming to about 6 GB, due to the amount of vectors it needed to store. Using the GloVe file allowed us to make our embedding matrix for the data.  
- Methods:
  - For our methodology we implemented embeddings in order to compare words and come up with a similarity score based on the cosine similarity of the phrases. First, we read in the data from the ‘train.csv’ file and printed out the number of unique items for the ‘anchor’ , ‘target’, and ‘context’ columns. From this there were 733 anchors, 29340 targets, and 106 contexts. This reveals that an anchor maps to many different targets and often can be in the same context. Next, we took in the data from the ‘titles.csv’ file and added it to our train data as an additional column. Then we removed unnecessary punctuation from the titles column of the data and unnecessary words. Afterwards we made the anchor and title into one sentence. This allowed for more words to be associated with each other when looking at our embeddings. Each word ws tokenized so that it could be represented by an integer value. Afterwards, we went through the GloVe file to create word vectors. Anytime a word was not in the GloVe file, it would be replaced with ‘<oov>’ in order to know that the word did not exist. Later on, those indexes would hold a value of 0. To finally get a predicted score, we used cosine_similarity on each of the words in both the target and the anchor. Doing this allowed us to get output in the range 0 to 1 and to base the output on the word vectors that we created. We also did tests using Pearson correlation and mean square error to recognize the efficiency of our result. 
Our approach is correct in that it establishes a relationship between words that goes beyond if they are just the same. Using GloVe embeddings allowed us to establish a similarity between words based on other words they were associated with. This allowed us to check the ‘closeness’ of the words. However, our method is not the best method because we were unable to establish a neural network to train the data. After creating our embeddings file, those values could be taken into the network. The layers of the neural network would have the embedding layer, two bidirectional LSTM layers, a fully connected layer, and an output layer. The bidirectional long-short term memory layers would allow for similarity to be determined with starting with the anchor and also with starting with the target. A fully connected layer would allow all the inputs to be connected to every activation function. The neural network would also be trained using Mean Square Error as the loss function. The weights would then be updated after each epoch. 
- Experiments:
  - In our embedding Colab file, we looked at Kaggle user, Himanshu Bag’s work to find how to intake the data given in the Kaggle competition and interpret it. The information in train.csv and test.csv was stored in a data frame with columns, ‘id’, ‘anchor’, ‘target’, ‘context’, ‘score’, and ‘title’. We took the cosine similarity between the patent names and phrases given in train.csv and test.csv. We stored this in the train_df[‘word_embed_score’] data frame column to compare to the actual train_df[‘score’]. We used the jax function jax.numpy.corrcoef() to find the correlation between the score variable and word_embed_score to see the accuracy of our tests. 
  - One of the tests we used was Pearson Correlation coefficient which is used to test the similarity for bivariate data. The values can range from -1 to 1. A value of 0 means there is no correlation between the data sets. A more negative value indicates a negative correlation, while a positive value indicates a positive correlation. Because the correlation was only 0.3349 from our testing, we can assume that the algorithm we used was not efficient at detecting the phrase to phrase similarities for patents. However, there was some positive correlation between the predicted scores and actual scores. 
  - Another way we tested our results is by looking at the Mean Square Error. Mean Square Error allows for us to determine the difference between the predicted and actual scores. The mean square error for the predicted vs actual scores is approximately .095. This indicates that the predicted and actual scores were near each other, but not perfectly accurate. This could be improved by feeding our predictions to a neural network and using MSE as the loss function in order to update the weights. 
  - ![image](https://user-images.githubusercontent.com/90022095/164999644-749655f8-5c33-462a-ab3f-d80099fbf958.png)
  - The graph above depicts the values of our predicted scores vs the actual scores for the first 100 patents. Based on this graph, there seems to be some correlation between the actual and calculated score, but they do not always follow the same pattern. This emphasizes the need for us to do further implementations. 
- Conclusion:
  - This project has established the importance of NLP in daily use. Using word2vec and GloVe embeddings revealed how word meaning could also be described using numbers. Unfortunately, there is not a lot of material on implementing a neural network that works for two phrase inputs. The available resources utilize high level APIs and methodology that greatly exceeds that of an introductory course. However, the basic principles of Natural Language Processing are instilled through our project. One of the main takeaways from this project is the representation of words as tokens and the need for outside data to determine semantic similarity. Initially, we attempted to do this project with the learning only coming from the given data, but that would not have been successful because it would not allow us to learn the correlation between words. For new applications of our project, we should implement a learner so that the accuracy will improve and implement word2vec over GloVe. 


References:
  
 - Bag, H. (2022, April 22). Patent matching-glove embd. Kaggle. Retrieved April 24, 2022, from https://www.kaggle.com/code/himanshubag/patent-matching-glove-embd
  
 - Fang, Lintao, et al. (2021, September 5). Patent2Vec: Multi-View Representation Learning on Patent-Graphs for Patent Classification. World Wide Web, vol. 24, no. 5, pp. 1791–812. Retrieved April 24, 2022, from EBSCOhost,https://doi-org.libdb.njit.edu:8443/10.1007/s11280-021-00885-4.
  
 - Jr., T. T. (2022, March 22). Creating word embeddings with jax. Medium. Retrieved April 24, 2022, from https://towardsdatascience.com/creating-word-embeddings-with-jax-c9f144901472
  
 - Schwartz , B. (2022, February 23). Welcome Bert: Google's latest search algorithm to better understand natural language. Search Engine Land. Retrieved April 24, 2022, from https://searchengineland.com/welcome-bert-google-artificial-intelligence-for-understanding-search-queries-323976 
